{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# change path to workspace\n",
    "os.chdir('/workspace')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# reset gpu memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "'''train'''\n",
    "parser.add_argument(\"--max_lr\", default=3e-4, type=float)\n",
    "parser.add_argument(\"--wd\", default=1e-5, type=float)\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--run_name\", default=None, type=Path)\n",
    "parser.add_argument('--loss_type', default=\"label_smooth\", type=str)\n",
    "parser.add_argument('--n_epochs', default=None, type=int)\n",
    "parser.add_argument('--epoch_mix', default=None, type=int)\n",
    "parser.add_argument(\"--amp\", action='store_true')\n",
    "parser.add_argument(\"--filter_bias_and_bn\", action='store_true', default=True)\n",
    "parser.add_argument(\"--ext_pretrained\", default=None, type=str)\n",
    "parser.add_argument(\"--multilabel\", action='store_true')\n",
    "parser.add_argument('--save_path', default=None, type=Path)\n",
    "parser.add_argument('--load_path', default=None, type=Path)\n",
    "parser.add_argument('--scheduler', default=None, type=str)\n",
    "parser.add_argument('--augs_signal', nargs='+', type=str,\n",
    "                    default=['amp', 'neg', 'tshift', 'tmask', 'ampsegment', 'cycshift'])\n",
    "parser.add_argument('--augs_noise', nargs='+', type=str,\n",
    "                    default=['awgn', 'abgn', 'apgn', 'argn', 'avgn', 'aun', 'phn', 'sine'])\n",
    "# parser.add_argument('--augs_signal', nargs='+', type=str,\n",
    "#                     default=[])\n",
    "# parser.add_argument('--augs_noise', nargs='+', type=str,\n",
    "#                     default=[])\n",
    "parser.add_argument('--augs_mix', nargs='+', type=str, default=['mixup', 'timemix', 'freqmix', 'phmix'])\n",
    "parser.add_argument('--mix_loss', default='bce', type=str)\n",
    "parser.add_argument('--mix_ratio', default=1, type=float)\n",
    "parser.add_argument('--ema', default=0.995, type=float)\n",
    "parser.add_argument('--log_interval', default=100, type=int)\n",
    "parser.add_argument(\"--kd_model\", default=None, type=Path)\n",
    "parser.add_argument(\"--use_bg\", action='store_true', default=False)\n",
    "parser.add_argument(\"--resume_training\", action='store_true', default=False)\n",
    "parser.add_argument(\"--use_balanced_sampler\", action='store_true', default=False)\n",
    "'''common'''\n",
    "parser.add_argument('--local_rank', default=0, type=int)\n",
    "parser.add_argument('--gpu_ids', nargs='+', default=[0])\n",
    "parser.add_argument(\"--use_ddp\", action='store_true')\n",
    "parser.add_argument(\"--use_dp\", action='store_true')\n",
    "parser.add_argument('--save_interval', default=100, type=int)\n",
    "'''data'''\n",
    "parser.add_argument('--fold_id', default=1, type=int)\n",
    "parser.add_argument(\"--data_subtype\", default='balanced', type=str)\n",
    "parser.add_argument('--seq_len', default=90112, type=int)\n",
    "parser.add_argument('--dataset', default=\"esc50\", type=str)\n",
    "parser.add_argument('--n_classes', default=50, type=int)\n",
    "'''net'''\n",
    "parser.add_argument('--ds_factors', nargs='+', type=int, default=[4, 4, 4, 4])\n",
    "parser.add_argument('--n_head', default=8, type=int)\n",
    "parser.add_argument('--n_layers', default=4, type=int)\n",
    "parser.add_argument(\"--emb_dim\", default=128, type=int)\n",
    "parser.add_argument(\"--model_type\", default='SoundNetRaw', type=str)\n",
    "parser.add_argument(\"--nf\", default=16, type=int)\n",
    "parser.add_argument(\"--dim_feedforward\", default=512, type=int)\n",
    "parser.add_argument(\"--sampling_rate\", default=22050, type=int)\n",
    "'''system'''\n",
    "parser.add_argument('--data_dir', default='data/', type=Path)\n",
    "parser.add_argument('--data_path', default='/workspace/data/ESC-50-master', type=str)\n",
    "\n",
    "parser.add_argument('--gpus', type=list, default=[0])\n",
    "parser.add_argument('--num_workers', type=int, default=30)\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESC50 Dataset\n",
    "The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification.\n",
    "\n",
    "The dataset consists of 5-second-long recordings organized into 50 semantical classes.\n",
    "[Github](https://github.com/karolpiczak/ESC-50)\n",
    "\n",
    "[Huggingface](https://huggingface.co/datasets/ashraq/esc50) \"ashraq/esc50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "Prepare your dataset. All code that only need to be executed ones before training and results fit on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 387M/387M [00:37<00:00, 10.2MB/s]\n",
      "Downloading data: 100%|██████████| 387M/387M [00:35<00:00, 11.0MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [01:12<00:00, 72.90s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 574.40it/s]\n",
      "Generating train split: 100%|██████████| 2000/2000 [00:01<00:00, 1394.30 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:09<00:00, 206.15 examples/s]\n",
      "Saving the dataset (3/3 shards): 100%|██████████| 2000/2000 [00:00<00:00, 3339.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "def preprocess_audio(example):\n",
    "    audio = example['audio']\n",
    "    audio = audio['array']\n",
    "    if audio.shape[0] >= args.seq_len:\n",
    "        max_audio_start = audio.shape[0] - args.seq_len\n",
    "        audio_start = random.randint(0, max_audio_start)\n",
    "        audio = audio[audio_start : audio_start + args.seq_len]\n",
    "    else:\n",
    "        audio = F.pad(\n",
    "            audio, (0, args.seq_len - audio.size(0)), \"constant\"\n",
    "        ).data\n",
    "    example['audio'] = audio\n",
    "    return example\n",
    "\n",
    "esc50 = load_dataset(\"ashraq/esc50\", cache_dir=args.data_dir)\n",
    "esc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=args.sampling_rate))\n",
    "# rename column target to label\n",
    "esc50 = esc50.rename_column(\"target\", \"label\")\n",
    "esc50 = esc50.map(preprocess_audio)\n",
    "esc50.save_to_disk(os.path.join(args.data_dir, 'esc50processed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import Audio\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.augmentations.audio_augs import AudioAugs\n",
    "\n",
    "\n",
    "\n",
    "def train_transform(batch):\n",
    "    transforms = args.augs_signal + args.augs_noise\n",
    "    audio = torch.Tensor(batch['audio'])\n",
    "    if transforms is not None:\n",
    "        # check if audio has more then 1 dimension\n",
    "        if len(audio.shape) > 1:\n",
    "            # iterate over all dimensions\n",
    "            for i in range(audio.shape[0]):\n",
    "                audio[i] = AudioAugs(transforms, args.sampling_rate, p=0.5)(audio[i])\n",
    "        else:\n",
    "            batch['audio'] = AudioAugs(transforms, args.sampling_rate, p=0.5)(audio)      \n",
    "    batch['audio'] = audio.unsqueeze(1)\n",
    "    batch['label'] = torch.Tensor(batch['label']).long()\n",
    "    return batch\n",
    "\n",
    "def gpu_transforms(batch):\n",
    "    transforms = args.augs_signal + args.augs_noise\n",
    "    audio = batch['audio']\n",
    "    if transforms is not None:\n",
    "        # check if audio has more then 1 dimension\n",
    "        if len(audio.shape) > 1:\n",
    "            # iterate over all dimensions\n",
    "            for i in range(audio.shape[0]):\n",
    "                audio[i] = AudioAugs(transforms, args.sampling_rate, p=0.5)(audio[i][0])\n",
    "        else:\n",
    "            batch['audio'] = AudioAugs(transforms, args.sampling_rate, p=0.5)(audio)      \n",
    "    batch['audio'] = audio\n",
    "    return batch\n",
    "    \n",
    "def test_transform(batch):\n",
    "    batch['audio'] = torch.Tensor(batch['audio']).unsqueeze(1)\n",
    "    batch['label'] = torch.Tensor(batch['label']).long()\n",
    "    return batch\n",
    "\n",
    "class ESC50DataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: Path = args.data_dir):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    # called only within a single process on CPU but everytime trainer is envoked\n",
    "    # def prepare_data(self):\n",
    "\n",
    "    # run on each GPU\n",
    "    def setup(self, stage: str):\n",
    "        # load from disk\n",
    "        esc50 = load_from_disk(os.path.join(self.data_dir, 'esc50processed'))\n",
    "        esc50 = esc50.remove_columns(['filename', 'fold', 'category', 'esc10', 'src_file', 'take'])\n",
    "        # split into train, val, test\n",
    "        esc50 = esc50['train'].train_test_split(test_size=0.2, shuffle=True)\n",
    "        self.dataset_train = esc50['train']\n",
    "        # self.dataset_train = esc50['train'].with_format('torch', columns=['audio', 'label'])\n",
    "        self.dataset_test = esc50['test'].with_format('torch', columns=['audio', 'label'])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        self.dataset_train.set_transform(train_transform)\n",
    "        # self.dataset_train.set_transform(test_transform)\n",
    "        return DataLoader(self.dataset_train, batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        # generator=torch.Generator(device='cuda')\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        self.dataset_test.set_transform(test_transform)\n",
    "        return DataLoader(self.dataset_test, batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        )\n",
    "    \n",
    "    # def on_after_batch_transfer(self, batch, dataloader_idx):\n",
    "    #     if self.trainer.training:\n",
    "    #         batch = gpu_transforms(batch)\n",
    "    #     return batch\n",
    "\n",
    "datamodule = ESC50DataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 387M/387M [00:24<00:00, 15.7MB/s]\n",
      "Downloading data: 100%|██████████| 387M/387M [00:24<00:00, 15.6MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:49<00:00, 49.30s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 553.63it/s]\n",
      "Generating train split: 100%|██████████| 2000/2000 [00:01<00:00, 1355.59 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filename', 'fold', 'target', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filename', 'fold', 'target', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Audio\n",
    "esc50 = load_dataset(\"ashraq/esc50\")\n",
    "esc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=args.sampling_rate))\n",
    "# split into train, val, test\n",
    "esc50 = esc50['train'].train_test_split(test_size=0.2, shuffle=True)\n",
    "esc50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.prepare_data()\n",
    "datamodule.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.augmentations.batch_augs import BatchAugs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "ba_params = {\n",
    "        'seq_len': args.seq_len,\n",
    "        'fs': args.sampling_rate,\n",
    "        'augs': args.augs_mix,\n",
    "        'device': device,\n",
    "        'mix_ratio': args.mix_ratio,\n",
    "        'batch_sz': args.local_rank,\n",
    "        'epoch_mix': args.epoch_mix,\n",
    "        'resample_factors': [0.8, 0.9, 1.1, 1.2],\n",
    "        'multilabel': True if args.multilabel else False,\n",
    "        'mix_loss': args.mix_loss\n",
    "    }\n",
    "batch_augs = BatchAugs(ba_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#####################\n",
    "# losses            #\n",
    "#####################\n",
    "if args.loss_type == \"label_smooth\":\n",
    "    from model.losses import LabelSmoothCrossEntropyLoss\n",
    "    criterion = LabelSmoothCrossEntropyLoss(smoothing=0.1, reduction='sum')\n",
    "elif args.loss_type == \"cross_entropy\":\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "elif args.loss_type == \"focal\":\n",
    "    from model.losses import FocalLoss\n",
    "    criterion = FocalLoss()\n",
    "elif args.loss_type == 'bce':\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "else:\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import lightning as L\n",
    "from model.soundnet import SoundNetRaw as SoundNet\n",
    "from utils.helper_funcs import accuracy\n",
    "\n",
    "class EAT(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(args)\n",
    "        ds_fac = np.prod(np.array(args.ds_factors)) * 4\n",
    "        self.model = SoundNet(\n",
    "                nf=args.nf,\n",
    "                dim_feedforward=args.dim_feedforward,\n",
    "                clip_length=args.seq_len // ds_fac,\n",
    "                embed_dim=args.emb_dim,\n",
    "                n_layers=args.n_layers,\n",
    "                nhead=args.n_head,\n",
    "                n_classes=args.n_classes,\n",
    "                factors=args.ds_factors,\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def augment_audio(audio):\n",
    "        transforms = args.augs_signal + args.augs_noise\n",
    "        for i in range(audio.shape[0]):\n",
    "            audio[i] = AudioAugs(transforms, args.sampling_rate, p=0.5)(audio[i])\n",
    "        return audio\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['audio']\n",
    "        y = batch['label']\n",
    "        # augment x\n",
    "        # x = augment_audio(x)\n",
    "        x, targets, is_mixed = batch_augs(x, y) # TODO: removed epoch parameter\n",
    "        pred = self(x)\n",
    "        if is_mixed:\n",
    "            loss_cls = batch_augs.mix_loss(pred, targets, n_classes=args.n_classes,\n",
    "            pred_one_hot=args.multilabel)\n",
    "        else:\n",
    "            loss_cls = criterion(pred, y)\n",
    "        self.log('loss_cls', loss_cls)\n",
    "        return loss_cls\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['audio']\n",
    "        y = batch['label']\n",
    "        pred = self(x)\n",
    "        loss_cls = criterion(pred, y)\n",
    "        acc = accuracy(pred, y, topk=(1,))[0]\n",
    "        self.log('acc', acc)\n",
    "        self.log('test_loss', loss_cls)\n",
    "        return loss_cls\n",
    "    \n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     loss = self.training_step(batch, batch_idx)\n",
    "    #     self.log('val_loss', loss)\n",
    "    #     return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if args.amp:\n",
    "            from torch.cuda.amp import GradScaler\n",
    "            scaler = GradScaler(init_scale=2**10)\n",
    "            eps = 1e-4\n",
    "        else:\n",
    "            scaler = None\n",
    "            eps = 1e-8\n",
    "        parameters = self.model.parameters()\n",
    "        return torch.optim.AdamW(parameters,\n",
    "                            lr=args.max_lr,\n",
    "                            betas=[0.9, 0.99],\n",
    "                            weight_decay=0,\n",
    "                            eps=eps)\n",
    "model = EAT()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 90112])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 50])\n"
     ]
    }
   ],
   "source": [
    "# get one sample from datamodule\n",
    "# datamodule.prepare_data()\n",
    "datamodule.setup(stage='fit')\n",
    "sample = next(iter(datamodule.train_dataloader()))\n",
    "x = sample['audio']\n",
    "y = sample['label']\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "pred = model(x)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "\n",
    "trainer = L.Trainer(max_epochs=10, accelerator='gpu', devices=args.gpus, log_every_n_steps=10) # set devices to a list of GPU ids to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | SoundNetRaw | 5.2 M \n",
      "--------------------------------------\n",
      "5.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 M     Total params\n",
      "20.722    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 12/12 [00:09<00:00,  1.25it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 12/12 [00:09<00:00,  1.24it/s, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "# start training \n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Test the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 13.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            acc            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     18.22916603088379     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     463.3260803222656     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m           acc           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    18.22916603088379    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    463.3260803222656    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'acc': 18.22916603088379, 'test_loss': 463.3260803222656}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
