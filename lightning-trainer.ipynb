{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramerers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "'''train'''\n",
    "parser.add_argument(\"--max_lr\", default=3e-4, type=float)\n",
    "parser.add_argument(\"--wd\", default=1e-5, type=float)\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--run_name\", default=None, type=Path)\n",
    "parser.add_argument('--loss_type', default=\"label_smooth\", type=str)\n",
    "parser.add_argument('--n_epochs', default=None, type=int)\n",
    "parser.add_argument('--epoch_mix', default=None, type=int)\n",
    "parser.add_argument(\"--amp\", action='store_true')\n",
    "parser.add_argument(\"--filter_bias_and_bn\", action='store_true', default=True)\n",
    "parser.add_argument(\"--ext_pretrained\", default=None, type=str)\n",
    "parser.add_argument(\"--multilabel\", action='store_true')\n",
    "parser.add_argument('--save_path', default=None, type=Path)\n",
    "parser.add_argument('--load_path', default=None, type=Path)\n",
    "parser.add_argument('--scheduler', default=None, type=str)\n",
    "# parser.add_argument('--augs_signal', nargs='+', type=str,\n",
    "                    # default=['amp', 'neg', 'tshift', 'tmask', 'ampsegment', 'cycshift'])\n",
    "parser.add_argument('--augs_signal', nargs='+', type=str,\n",
    "                    default=[])\n",
    "# parser.add_argument('--augs_noise', nargs='+', type=str,\n",
    "#                     default=['awgn', 'abgn', 'apgn', 'argn', 'avgn', 'aun', 'phn', 'sine'])\n",
    "parser.add_argument('--augs_noise', nargs='+', type=str,\n",
    "                    default=[])\n",
    "parser.add_argument('--augs_mix', nargs='+', type=str, default=['mixup', 'timemix', 'freqmix', 'phmix'])\n",
    "parser.add_argument('--mix_loss', default='bce', type=str)\n",
    "parser.add_argument('--mix_ratio', default=1, type=float)\n",
    "parser.add_argument('--ema', default=0.995, type=float)\n",
    "parser.add_argument('--log_interval', default=100, type=int)\n",
    "parser.add_argument(\"--kd_model\", default=None, type=Path)\n",
    "parser.add_argument(\"--use_bg\", action='store_true', default=False)\n",
    "parser.add_argument(\"--resume_training\", action='store_true', default=False)\n",
    "parser.add_argument(\"--use_balanced_sampler\", action='store_true', default=False)\n",
    "'''common'''\n",
    "parser.add_argument('--local_rank', default=0, type=int)\n",
    "parser.add_argument('--gpu_ids', nargs='+', default=[0])\n",
    "parser.add_argument(\"--use_ddp\", action='store_true')\n",
    "parser.add_argument(\"--use_dp\", action='store_true')\n",
    "parser.add_argument('--save_interval', default=100, type=int)\n",
    "'''data'''\n",
    "parser.add_argument('--fold_id', default=1, type=int)\n",
    "parser.add_argument(\"--data_subtype\", default='balanced', type=str)\n",
    "parser.add_argument('--seq_len', default=90112, type=int)\n",
    "parser.add_argument('--dataset', default=\"urban8k\", type=str)\n",
    "parser.add_argument('--n_classes', default=50, type=int)\n",
    "'''net'''\n",
    "parser.add_argument('--ds_factors', nargs='+', type=int, default=[4, 4, 4, 4])\n",
    "parser.add_argument('--n_head', default=8, type=int)\n",
    "parser.add_argument('--n_layers', default=4, type=int)\n",
    "parser.add_argument(\"--emb_dim\", default=128, type=int)\n",
    "parser.add_argument(\"--model_type\", default='SoundNetRaw', type=str)\n",
    "parser.add_argument(\"--nf\", default=16, type=int)\n",
    "parser.add_argument(\"--dim_feedforward\", default=512, type=int)\n",
    "parser.add_argument(\"--sampling_rate\", default=22050, type=int)\n",
    "'''system'''\n",
    "parser.add_argument('--data_dir', default='data/', type=Path)\n",
    "parser.add_argument('--gpus', type=list, default=[0])\n",
    "parser.add_argument('--num_workers', type=int, default=32)\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESC50 Dataset\n",
    "The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification.\n",
    "\n",
    "The dataset consists of 5-second-long recordings organized into 50 semantical classes.\n",
    "[Github](https://github.com/karolpiczak/ESC-50)\n",
    "\n",
    "[Huggingface](https://huggingface.co/datasets/ashraq/esc50) \"ashraq/esc50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import Audio\n",
    "\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from workspace.datasets.audio_augs import AudioAugs\n",
    "\n",
    "def preprocess_audio(example):\n",
    "    audio = example['audio']\n",
    "    samling_rate = audio['sampling_rate']\n",
    "    audio = audio['array']\n",
    "    if audio.shape[0] >= args.seq_len:\n",
    "        max_audio_start = audio.shape[0] - args.seq_len\n",
    "        audio_start = random.randint(0, max_audio_start)\n",
    "        audio = audio[audio_start : audio_start + args.seq_len]\n",
    "    else:\n",
    "        audio = F.pad(\n",
    "            audio, (0, args.seq_len - audio.size(0)), \"constant\"\n",
    "        ).data\n",
    "    example['audio'] = audio\n",
    "    return example\n",
    "\n",
    "def transform(batch):\n",
    "    transforms = args.augs_signal + args.augs_noise\n",
    "    audio = torch.Tensor(batch['audio'])\n",
    "    if transforms is not None:\n",
    "        batch['audio'] = AudioAugs(transforms, args.sampling_rate, p=0.5)(audio)      \n",
    "    batch['audio'] = audio.unsqueeze(0)\n",
    "    return batch\n",
    "    \n",
    "\n",
    "class ESC50DataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: Path = args.data_dir):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    # called only within a single process on CPU\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        esc50 = load_dataset(\"ashraq/esc50\", cache_dir=self.data_dir)\n",
    "        esc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=args.sampling_rate))\n",
    "        # rename column target to label\n",
    "        esc50 = esc50.rename_column(\"target\", \"label\")\n",
    "        esc50 = esc50.map(preprocess_audio)\n",
    "        esc50.save_to_disk(os.path.join(self.data_dir, 'esc50processed'))\n",
    "\n",
    "        \n",
    "\n",
    "    # run on each GPU\n",
    "    def setup(self, stage: str):\n",
    "        # load from disk\n",
    "        esc50 = load_from_disk(os.path.join(self.data_dir, 'esc50processed'))\n",
    "        esc50 = esc50.remove_columns(['filename', 'fold', 'category', 'esc10', 'src_file', 'take'])\n",
    "        # split into train, val, test\n",
    "        esc50 = esc50['train'].train_test_split(test_size=0.2, shuffle=True)\n",
    "        self.dataset_test = esc50['test'].with_format('torch', columns=['audio', 'label'])\n",
    "        self.dataset_train = esc50['train'].with_format('torch', columns=['audio', 'label'])\n",
    "        self.dataset_test.set_transform(transform)\n",
    "        self.dataset_train.set_transform(transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train, batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "         return DataLoader(self.dataset_test, batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "         return DataLoader(self.dataset_test, batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        )\n",
    "\n",
    "datamodule = ESC50DataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filename', 'fold', 'target', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filename', 'fold', 'target', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Audio\n",
    "esc50 = load_dataset(\"ashraq/esc50\")\n",
    "esc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=args.sampling_rate))\n",
    "# split into train, val, test\n",
    "esc50 = esc50['train'].train_test_split(test_size=0.2, shuffle=True)\n",
    "esc50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column target to label\n",
    "esc50 = esc50.rename_column(\"target\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_test = esc50['test'].with_format('torch', columns=['audio', 'label'])\n",
    "dataset_train = esc50['train'].with_format('torch', columns=['audio', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'fold', 'label', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "    num_rows: 1600\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(39),\n",
       " 'audio': tensor([-0.0017,  0.0006, -0.0022,  ...,  0.0000,  0.0000,  0.0000])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_audio(example):\n",
    "    audio = example['audio']\n",
    "    samling_rate = audio['sampling_rate']\n",
    "    audio = audio['array']\n",
    "    if audio.shape[0] >= args.seq_len:\n",
    "        max_audio_start = audio.size(0) - args.seq_len\n",
    "        audio_start = random.randint(0, max_audio_start)\n",
    "        audio = audio[audio_start : audio_start + args.seq_len]\n",
    "    else:\n",
    "        audio = F.pad(\n",
    "            audio, (0, args.seq_len - audio.size(0)), \"constant\"\n",
    "        ).data\n",
    "    example['audio'] = audio\n",
    "    return example\n",
    "\n",
    "preprocess_audio(dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(39),\n",
       " 'audio': {'path': None,\n",
       "  'array': tensor([ 3.5112e-07, -3.0661e-07,  2.1097e-07,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]),\n",
       "  'sampling_rate': tensor(22050)}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datamodule.prepare_data()\n",
    "datamodule.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 26,\n",
       " 'audio': tensor([[-0.0021, -0.0028, -0.0052,  ..., -0.0028, -0.0030,  0.0002]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datamodule.dataset_train\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 90112])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0:10]['audio'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([ 3,  0, 24, 12, 47,  9, 26,  9, 15, 44, 35,  6, 46, 43,  5, 15,  0, 36,\n",
       "         22,  5, 31,  1, 33, 32, 43, 49, 41, 43, 32, 11, 10,  4, 47, 36, 29, 38,\n",
       "         12, 41, 37, 14, 42, 20, 32,  7, 48, 32, 12, 10, 30,  4, 14, 20, 14, 35,\n",
       "         32, 37, 40, 35, 28, 33, 25, 14, 17,  1,  1,  5, 17,  3, 49, 31, 13, 31,\n",
       "          9, 27, 20, 30, 10, 29, 17, 22, 22, 15, 48, 48, 17,  0, 18,  4, 14, 20,\n",
       "         42, 35,  2, 39,  2,  8, 42,  9, 48, 39, 26,  2,  0,  6, 49, 18, 33, 27,\n",
       "          6, 43,  8,  1, 18, 35, 28, 41, 24, 12, 29, 15, 17, 35, 48, 12, 19,  0,\n",
       "         28, 41]),\n",
       " 'audio': tensor([[[-4.1832e-01, -3.8508e-01, -3.1146e-01,  ...,  2.8594e-01,\n",
       "            3.3623e-01,  3.2967e-01]],\n",
       " \n",
       "         [[ 7.5707e-02,  1.1831e-01,  1.4611e-01,  ..., -1.9797e-02,\n",
       "           -1.6428e-02, -1.3772e-02]],\n",
       " \n",
       "         [[-2.3252e-01, -3.0039e-01, -3.8625e-01,  ..., -5.2688e-03,\n",
       "           -2.7096e-03,  5.9776e-06]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.7497e-03, -1.8858e-03, -1.7380e-03,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 7.0383e-05,  1.7986e-04,  7.9244e-05,  ..., -1.0429e-02,\n",
       "           -5.1537e-03,  1.7461e-03]],\n",
       " \n",
       "         [[-3.4222e-03, -1.3108e-02, -1.1182e-02,  ...,  9.1802e-02,\n",
       "            8.6154e-02,  4.0154e-02]]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = datamodule.train_dataloader()\n",
    "first_batch = next(iter(train_dataloader))\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 90112])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch['audio'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0050,  0.0210,  0.0506,  ...,  0.0094,  0.0280,  0.0201],\n",
       "        [ 0.0003,  0.0004,  0.0004,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0018,  0.0809,  0.0700,  ...,  0.0968,  0.1595,  0.2512],\n",
       "        ...,\n",
       "        [ 0.0051, -0.0120, -0.0181,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0024,  0.0027,  0.0022,  ..., -0.0243, -0.0349, -0.0430],\n",
       "        [-0.0013, -0.0039, -0.0126,  ...,  0.0528,  0.0516,  0.0475]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = first_batch['audio']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([ 8, 34, 41,  7, 15, 41, 17, 37, 38, 35, 19, 20, 22, 23, 21, 39, 44, 21,\n",
       "         27, 18, 15, 40, 32,  9, 35, 48, 31, 34,  4, 22, 21, 32, 47, 49, 26, 49,\n",
       "         40, 30, 23, 16, 48, 32, 13, 39, 28,  3, 37, 31, 42, 41, 25,  1, 24, 47,\n",
       "         40,  8, 49,  9, 10, 44, 15, 37, 49, 41, 26, 13, 20, 25, 34,  8, 39, 10,\n",
       "         44, 17,  2,  9, 46,  5, 34, 47, 16, 18, 44, 29, 36,  4,  8,  3, 11, 32,\n",
       "          2, 17, 16,  5, 42, 45, 21, 33, 37, 36,  8, 23, 41, 18, 28, 41, 44, 16,\n",
       "         48, 36, 13, 43, 27, 16, 24,  6, 18, 13, 49, 14, 11,  8, 20,  0, 22,  1,\n",
       "         37, 35]),\n",
       " 'audio': tensor([[-0.0050,  0.0210,  0.0506,  ...,  0.0094,  0.0280,  0.0201],\n",
       "         [ 0.0003,  0.0004,  0.0004,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0018,  0.0809,  0.0700,  ...,  0.0968,  0.1595,  0.2512],\n",
       "         ...,\n",
       "         [ 0.0051, -0.0120, -0.0181,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0024,  0.0027,  0.0022,  ..., -0.0243, -0.0349, -0.0430],\n",
       "         [-0.0013, -0.0039, -0.0126,  ...,  0.0528,  0.0516,  0.0475]])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from workspace.datasets.batch_augs import BatchAugs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "ba_params = {\n",
    "        'seq_len': args.seq_len,\n",
    "        'fs': args.sampling_rate,\n",
    "        'augs': args.augs_mix,\n",
    "        'device': device,\n",
    "        'mix_ratio': args.mix_ratio,\n",
    "        'batch_sz': args.local_rank,\n",
    "        'epoch_mix': args.epoch_mix,\n",
    "        'resample_factors': [0.8, 0.9, 1.1, 1.2],\n",
    "        'multilabel': True if args.multilabel else False,\n",
    "        'mix_loss': args.mix_loss\n",
    "    }\n",
    "batch_augs = BatchAugs(ba_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#####################\n",
    "# losses            #\n",
    "#####################\n",
    "if args.loss_type == \"label_smooth\":\n",
    "    from modules.losses import LabelSmoothCrossEntropyLoss\n",
    "    criterion = LabelSmoothCrossEntropyLoss(smoothing=0.1, reduction='sum')\n",
    "elif args.loss_type == \"cross_entropy\":\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "elif args.loss_type == \"focal\":\n",
    "    from modules.losses import FocalLoss\n",
    "    criterion = FocalLoss()\n",
    "elif args.loss_type == 'bce':\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "else:\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import lightning as L\n",
    "from workspace.datasets.batch_augs import BatchAugs\n",
    "from modules.soundnet import SoundNetRaw as SoundNet\n",
    "\n",
    "class EAT(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(args)\n",
    "        ds_fac = np.prod(np.array(args.ds_factors)) * 4\n",
    "        self.model = SoundNet(\n",
    "                nf=args.nf,\n",
    "                dim_feedforward=args.dim_feedforward,\n",
    "                clip_length=args.seq_len // ds_fac,\n",
    "                embed_dim=args.emb_dim,\n",
    "                n_layers=args.n_layers,\n",
    "                nhead=args.n_head,\n",
    "                n_classes=args.n_classes,\n",
    "                factors=args.ds_factors,\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['audio']\n",
    "        y = batch['label']\n",
    "        x, targets, is_mixed = batch_augs(x, y) # TODO: removed epoch parameter\n",
    "        pred = self(x)\n",
    "        if is_mixed:\n",
    "            loss_cls = batch_augs.mix_loss(pred, targets, n_classes=args.n_classes,\n",
    "            pred_one_hot=args.multilabel)\n",
    "        else:\n",
    "            loss_cls = criterion(pred, y)\n",
    "        self.log('loss_cls', loss_cls)\n",
    "        return loss_cls\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['audio']\n",
    "        y = batch['label']\n",
    "        pred = self(x)\n",
    "        loss_cls = criterion(pred, y)\n",
    "        self.log('test_loss', loss_cls)\n",
    "        return loss_cls\n",
    "    \n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     loss = self.training_step(batch, batch_idx)\n",
    "    #     self.log('val_loss', loss)\n",
    "    #     return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if args.amp:\n",
    "            from torch.cuda.amp import GradScaler\n",
    "            scaler = GradScaler(init_scale=2**10)\n",
    "            eps = 1e-4\n",
    "        else:\n",
    "            scaler = None\n",
    "            eps = 1e-8\n",
    "        parameters = self.model.parameters()\n",
    "        return torch.optim.AdamW(parameters,\n",
    "                            lr=args.max_lr,\n",
    "                            betas=[0.9, 0.99],\n",
    "                            weight_decay=0,\n",
    "                            eps=eps)\n",
    "model = EAT()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 90112])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7032,  0.2245, -0.4650,  ...,  0.1418,  0.3108, -0.3904],\n",
       "        [ 0.3579, -1.0132, -0.0413,  ...,  0.1003, -0.0456,  0.0107],\n",
       "        [ 1.0528,  0.5265, -0.7309,  ..., -0.0906,  0.1717, -0.2026],\n",
       "        ...,\n",
       "        [ 0.9733, -0.1774, -0.4244,  ..., -0.2977, -0.0049, -0.0283],\n",
       "        [ 0.6691, -0.7178,  0.3668,  ..., -0.0950, -0.1738,  0.5077],\n",
       "        [ 0.1911, -0.3720,  0.2180,  ...,  0.2844,  0.6657, -0.1125]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get one sample from datamodule\n",
    "# datamodule.prepare_data()\n",
    "datamodule.setup(stage='fit')\n",
    "sample = next(iter(datamodule.train_dataloader()))\n",
    "x = sample['audio']\n",
    "print(x.shape)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "\n",
    "\n",
    "trainer = L.Trainer(max_epochs=1, accelerator='gpu', devices=args.gpus) # set devices to a list of GPU ids to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/lightning/pytorch/trainer/configuration_validator.py:68: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Saving the dataset (3/3 shards): 100%|██████████| 2000/2000 [00:03<00:00, 626.27 examples/s]\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | SoundNetRaw | 5.2 M \n",
      "--------------------------------------\n",
      "5.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 M     Total params\n",
      "20.722    Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.9/dist-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 12/12 [00:15<00:00,  1.25s/it, v_num=13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 12/12 [00:15<00:00,  1.27s/it, v_num=13]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start training \n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Test the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Saving the dataset (3/3 shards): 100%|██████████| 2000/2000 [00:01<00:00, 1023.78 examples/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 12.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     506.4825744628906     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    506.4825744628906    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 506.4825744628906}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
